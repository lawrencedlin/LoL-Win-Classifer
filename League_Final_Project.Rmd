
---
title: "Classifying Winning or Losing League of Legends Teams"
author: "Lawrence Lin Perm# 6294656"
date: "6/4/2020"
output:
  html_document:
    df_print: paged
  bookdown::pdf_book: null
  pdf_document:
    citation_package: natbib
bibliography: reff.bib
biblio-style: apalike
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
library(randomForest)
library(ROCR)
library(magrittr)
library(qwraps2)
library(knitr)
library(plyr)
library(boot)
library(ggpubr)

```

# I. Introduction
 League of Legends, commonly abbreviated LoL, is a popular video game with millions of concurrent players. In its premier 5v5 competitive mode, the objective is to destroy the opposing team's nexus. In this report, we will attempt to classify whether a given team wins or loses using measures of in-game performance.

## A Brief Primer on LoL
  In a LoL game, each player selects a unique champion to control for the duration of the game, which is usually from 20-40 minutes. The goal is to destroy the opposing team's nexus, which are located on in the enemy base protected by layers of towers which damage enemies that walk within a certain radius. Minions spawn from each nexus and walk down one of the three lanes on the game map and attack enemy minions, champions, and towers.  
![](leaguemap.jpeg)

  Each player that occupies a lane (or jungle where neutral minions spawn) will kill enemy minions and enemy champions that give gold that can be used to buy items in their base that strengthen their champion. There are also two neutral objectives along the centerline of the map that grant teamwide buffs that make killing enemy minions, champions, and towers easier. There are three towers in each lane that must be destroyed in order from outermost to innermost. After the third and innermost tower of a lane is destroyed, the inhibitor is targetable in that lane. The destruction of the inhibitor causes the minions in that lane to become empowered and destroy enemy minions and towers with ease. The two nexus turrets also become targetable. When the nexus turrets go down, then the enemy nexus is vulnerable and the team that destroys the nexus wins.
## Research Question
   I am interested in investigating how beneficial variables such as enemy champion kills, vision, and experience are for League of Legends teams. My research question is this: How accurately can we classify a team as a winner or loser looking at the overall performance of the team?

# II. Data 
  I obtained my data from kaggle. The data is from ranked League of Legends games from the three highest LoL ranks: Challenger, GrandMaster, and Master. These top three divisions comprise of the top 0.1% League players in the world [@Percentage].

```{r ,include=FALSE}
# Import and Aggregate Data
Challenger <- read.csv('Challenger_Ranked_Games.csv')

GrandMaster <- read.csv('GrandMaster_Ranked_Games.csv')

Master <- read.csv('Master_Ranked_Games.csv')

HighElo <- subset(rbind(Challenger,GrandMaster,Master),select = -c(gameId))

# Discard data for less than 900 seconds, which is the earliest time a team can surrender, and add a variable for game duration in minutes
HighElo <- filter(HighElo, gameDuraton >= 900)
HighElo <- mutate(HighElo,MinGameDuration = gameDuraton/60)
                   
```

  The data comes in the form of three csv files. There is one csv file for each division: Challenger, Grandmaster, and Master. I have combined each of the csv files into the dataframe HighElo. Included in HighElo are 48 predictors with 24 being unique to each team, and then the variable gameDuraton which is equal for both teams. We exclude the column gameId as it has no bearing on the outcome of the game.We cam see that there are a total of 49 predictor variables, with 47 being integers and 2 being numeric. 

```{r data overview, echo=FALSE, results = 'asis'}
#Dimensions of data
kable(data.frame(Rows = dim(HighElo)[1],Columns = dim(HighElo)[2]),caption = 'Dimensions of HighElo', col.names = c('Rows','Columns'))

#Predictor variables: Count and type
data.class <- sapply(HighElo,class)
kable(count(data.class),caption ='Predictor variables: Count and type, High Elo', col.names = c('Type','Frequency') )

#Missingness of data
missingness <- sapply(HighElo, function(y)
sum(length(which(is.na(y)))))
kable( sum(is.na(HighElo)), col.names = 'Missing Observations', caption='Missingness of HighElo')
```

Because the original data does not account for game length for each metric, we will be creating new features that do that and visualize them in the Methods section. I have already added a column to convert the game duration to minutes which we will use to calculate metrics based on the difference in gold, champion damage, objective damage, and more per minute to equate for game length. We have also discarded all games less than 15 minutes as that is the earliest possible time a team can surrender.
# III. Methods
  
## Analysis Plan
  My analysis plan is to use supervised machine learning to classify a given LoL team to the labels "Win" or "Lose". Because this is a binary classification problem, I believe it would be appropriate to train a logistic regression model and a Random Forest models on the data. For the Logistic Regression Model, I will find the optimal threshold value for classification by calculating the smallest euclidean distance between $(FPR, FNR)$ and $(0,0)$. For the Random Forest model, I will tune the parameter mtry using the function tuneRF(). My final model will be the lower of the validation error of 10-Fold Cross Validation on the Logistic Regression, or the Out-of-Bag error rate for the Random Forest (Cross-validation is not necessary since the Out-of-Bag error rate approximates the validation error).
  
   I will randomly choose between whether I am classifying the binary win/loss outcome variable for blue side or red side.  My response variable will be the binary outcome variable Win, which equals 1 if the selected team wins and 0 if they lose.  

  I made a 80/20 split for the Training and Test Sets, which leaves us with 148,454 observations in the training set and 37,114 observations in the test set.I designated a 50/50 split of the Training and Test sets analyzing the game from the blue side and red side. I then created new features GoldDiffPerMin, WardPlaceDiffPerMin,HealDiffPerMin, and other binary variables that only tracked the condition of the variable for the team chosen. I concatenated Blue and Red dataframes together to get my new Training and Test datasets. 
  
  
```{r Training/Test Split}

#Training and Test splits
set.seed(1)
SampleIndex <- sample(1:nrow(HighElo),0.80*nrow(HighElo))
Train <- HighElo[SampleIndex,]
Test <- HighElo[-SampleIndex,]

# Sample 50% of games from blue side for training and test set
set.seed(2)
SampleBlueTrain <- sample(1:nrow(Train),0.5*nrow(Train))
SampleBlueTest <- sample(1:nrow(Test),0.5*nrow(Test))

#Create Blue training set and Red training set
BlueTrain <- Train[SampleBlueTrain,]
RedTrain <- Train[-SampleBlueTrain,]

```

```{r Features Train, include=FALSE}
# Feature Creation for Blue and Red Training sets
BlueTrain <- mutate(BlueTrain, GoldDiffPerMin = (blueTotalGold - redTotalGold)/MinGameDuration,
                    WardPlaceDiffPerMin = (blueWardPlaced - redWardPlaced)/MinGameDuration,
                    DamageDiffPerMin = (blueChampionDamageDealt - redChampionDamageDealt)/MinGameDuration,
                    HealDiffPerMin = (blueTotalHeal - redTotalHeal)/MinGameDuration,
                    ObjectDmgDiffPerMin = (blueObjectDamageDealt - redObjectDamageDealt)/MinGameDuration,
                    FirstTower = blueFirstTower,
                    FirstBaron = blueFirstBaron,
                    FirstInhibitor = blueFirstInhibitor,
                    FirstDragon = blueFirstDragon,
                    Win = blueWins)
RedTrain <- mutate(RedTrain, GoldDiffPerMin = (redTotalGold - blueTotalGold)/MinGameDuration,
                    WardPlaceDiffPerMin = (redWardPlaced - blueWardPlaced)/MinGameDuration,
                    DamageDiffPerMin = (redChampionDamageDealt - blueChampionDamageDealt)/MinGameDuration,
                    HealDiffPerMin = (redTotalHeal - blueTotalHeal)/MinGameDuration,
                    ObjectDmgDiffPerMin = (redObjectDamageDealt - blueObjectDamageDealt)/MinGameDuration,
                    FirstTower = redFirstTower,
                    FirstBaron = redFirstBaron,
                    FirstInhibitor = redFirstInhibitor,
                    FirstDragon = redFirstDragon,
                    Win = redWins)
```

```{r Train, echo=FALSE}
# Combine Blue and Red Training sets back into one set using new predictors
Train <- rbind(BlueTrain,RedTrain)

# Create Train response and Train predictors
Train <- select(Train,MinGameDuration:Win)
Train$Win <- as.factor(Train$Win)
TrainY <- select(Train, Win)
TrainX <- select(Train, -Win)

#Create Blue test set and Red test set
BlueTest <- Test[SampleBlueTest,]
RedTest <- Test[-SampleBlueTest,]

```

```{r Features Test, include=FALSE}
BlueTest <- mutate(BlueTest, GoldDiffPerMin = (blueTotalGold - redTotalGold)/MinGameDuration,
                    WardPlaceDiffPerMin = (blueWardPlaced - redWardPlaced)/MinGameDuration,
                    DamageDiffPerMin = (blueChampionDamageDealt - redChampionDamageDealt)/MinGameDuration,
                    HealDiffPerMin = (blueTotalHeal - redTotalHeal)/MinGameDuration,
                    ObjectDmgDiffPerMin = (blueObjectDamageDealt - redObjectDamageDealt)/MinGameDuration,
                    FirstTower = blueFirstTower,
                    FirstBaron = blueFirstBaron,
                    FirstInhibitor = blueFirstInhibitor,
                    FirstDragon = blueFirstDragon,
                    Win = blueWins)
RedTest <- mutate(RedTest, GoldDiffPerMin = (redTotalGold - blueTotalGold)/MinGameDuration,
                    WardPlaceDiffPerMin = (redWardPlaced - blueWardPlaced)/MinGameDuration,
                    DamageDiffPerMin = (redChampionDamageDealt - blueChampionDamageDealt)/MinGameDuration,
                    HealDiffPerMin = (redTotalHeal - blueTotalHeal)/MinGameDuration,
                    ObjectDmgDiffPerMin = (redObjectDamageDealt - blueObjectDamageDealt)/MinGameDuration,
                    FirstTower = redFirstTower,
                    FirstBaron = redFirstBaron,
                    FirstInhibitor = redFirstInhibitor,
                    FirstDragon = redFirstDragon,
                    Win = redWins)
```

```{r Test}

# Combine Blue and Red Test sets back into one set using new predictors
Test <- rbind(BlueTest,RedTest)

# Create Train response and Train predictors
Test <- select(Test,MinGameDuration:Win)
Test$Win <- as.factor(Test$Win)

TestY <- select(Test, Win)
TestX <- select(Test, -Win)

```

## Data Overview of new features

The basic attributes of the modified set of all the data are summarized below, along with the five-number summary of the continuous variables. I have summarized the counts of the binary variables as well. The average game time is 24.89 minutes. We can see that most of the predictors have long tails in their distributions.

```{r AllData, echo=FALSE}
AllData <- rbind(Train,Test)
# str(AllData)
summaries <- lapply(AllData,summary)
summaries[1:6]
```
Here is a data overview of the new features created from the original data.

```{r New features, echo=FALSE, results = 'asis'}

#Dimensions of data
kable(data.frame(Rows = dim(AllData)[1],Columns = dim(AllData)[2]),caption = 'Dimensions of All Data', col.names = c('Rows','Columns'))

data.class.2 <- sapply(AllData,class)
kable(count(data.class.2),caption ='Predictor variables: Count and type, All Data', col.names = c('Type','Frequency') )

#Missingness of data
missingness <- sapply(AllData, function(y)
sum(length(which(is.na(y)))))
kable(sum(is.na(AllData)),col.names = c('Missing Observations'), caption='Missingness of All Data')

# Counts of binary Variables
kable(matrix(c(92340,93228,135932,49636,115699,69869,99770,85798,92837,92731), nrow = 2, ncol = 5, dimnames = list(c('0','1'),c('FirstTower','FirstBaron','FirstInhibitor','FirstDragon','Win'))), caption = 'Frequencies for binary variables')

```

I have created the boxplots of the response variable Win vs the continuous predictors to get infer their relationships to the response. From the boxplots it appears that GoldDiffPerMin and ObjectDmgDiffPerMin are usually higher for winning teams. 

```{r Boxplots, echo=FALSE}
# Boxplots

a <- ggplot(data = AllData,aes(x = Win, y = GoldDiffPerMin,color = Win)) +
  geom_boxplot()
b <- ggplot(data = AllData,aes(x = Win, y = WardPlaceDiffPerMin,color = Win)) +
  geom_boxplot()
c <- ggplot(data = AllData,aes(x = Win, y = DamageDiffPerMin,color = Win)) +
  geom_boxplot()
d <- ggplot(data = AllData,aes(x = Win, y = ObjectDmgDiffPerMin,color = Win)) +
  geom_boxplot()
e <- ggplot(data = AllData,aes(x = Win, y = HealDiffPerMin,color = Win)) +
  geom_boxplot()

ggarrange(a,b,c,d,e)



```

# III. Model Building

## Logistic Regression Model Building
I fit a Logistic Regression Model using the glm() function, to model the probability of Win on the predictors. The summary shows that all of the coefficients are positive except FirstTower and FirstBaron. This can be interpreted as meaning that the log-odds of winning are increased by all of our predictors except FirstTower and FirstBaron, where the log-odds of winning decrease when they are equal to 1. This goes against what we see  in our initial visualizations of the data, where I plotted the response vs. the predictors, which showed that FirstTower and FirstBaron was more often 1 for the winning teams. It is possible the the estimates of the parameters of my Logistic Regression model are unstable, because the classes are well-separated; in particular, GoldDiffPerMin is a very good predictor of Win. After performing 10-Fold Cross Validation, my validation error is 1.51%.

```{r Method Log Regression, echo = FALSE, message =FALSE, warning = FALSE}
# Fit a logistic regression model
glm.fit <- glm(Win ~. - MinGameDuration, family = binomial, data = Train)
suppressWarnings(summary(glm.fit))
# 10-Fold Cross Validation 
cvfit <- cv.glm(Train,glm.fit,K = 10)

print('Cross Validation Error')
cvfit$delta[[1]]


```

## Logistic Regression Model Analysis
  First I plotted the predictors versus the estimated probabilities of the training data. It appears that past a positive threshold value of GoldDiffPerMin, the estimated probability is always about 1. For the other predictors, demonstrating that it is indeed a well-separated class. WardPlaceDiffPerMin is similar except past a positive threeshold value, the estimated probability of winning is 0.
  
  In general, the estimated probability of winning is slightly higher for the teams that capture the First Baron, First Tower, and First Inhibitor.
 
```{r Visualize Log, echo=FALSE}
predict.glm.train <- predict(glm.fit, type = 'response')
col <- cut(predict.glm.train, breaks = c(0,0.5,1),labels = c('<= 0.5','>0.5'))

f <- ggplot(data = cbind(Train,predict.glm.train,col),aes(x=Train$GoldDiffPerMin, y=predict.glm.train, color = col)) +
  geom_point() +
  labs(y = 'Estimated probability of Winning')

g <- ggplot(data = cbind(Train,predict.glm.train,col),aes(x=Train$WardPlaceDiffPerMin, y=predict.glm.train, color = col)) +
  geom_point() +
  labs(y = 'Estimated probability of Winning')

h <- ggplot(data = cbind(Train,predict.glm.train,col),aes(x=Train$DamageDiffPerMin, y=predict.glm.train, color = col)) +
  geom_point() +
  labs(y = 'Estimated probability of Winning')

i <- ggplot(data = cbind(Train,predict.glm.train,col),aes(x=Train$HealDiffPerMin, y=predict.glm.train, color = col)) +
  geom_point() +
  labs(y = 'Estimated probability of Winning')

j <- ggplot(data = cbind(Train,predict.glm.train,col),aes(x=Train$ObjectDmgDiffPerMin, y=predict.glm.train, color = col)) +
  geom_point() +
  labs(y = 'Estimated probability of Winning')

k <- ggplot(data = cbind(Train,predict.glm.train),aes(x=as.factor(Train$FirstTower), y=predict.glm.train, color = as.factor(Train$FirstTower))) +
  geom_boxplot() +
  labs(y = 'Estimated probability of Winning')

l <- ggplot(data = cbind(Train,predict.glm.train),aes(x=as.factor(Train$FirstBaron), y=predict.glm.train, color = as.factor(Train$FirstBaron))) +
  geom_boxplot() +
  labs(y = 'Estimated probability of Winning')

m <- ggplot(data = cbind(Train,predict.glm.train),aes(x=as.factor(Train$FirstInhibitor), y=predict.glm.train, color = as.factor(Train$FirstInhibitor))) +
  geom_boxplot() +
  labs(y = 'Estimated probability of Winning')

n <- ggplot(data = cbind(Train,predict.glm.train),aes(x=as.factor(Train$FirstDragon), y=predict.glm.train, color = as.factor(Train$FirstDragon))) +
  geom_boxplot() +
  labs(y = 'Estimated probability of Winning')

ggarrange(f,g,h,i,j)

ggarrange(k,l,m,n)

predict.glm <- predict(glm.fit,newdata = Test, type = 'response')

```
 
 I created an AUC graph and a ROC graph for the logistic regression on Win. From the performance() function we can see that the optimal threshold is 0.48, so we should set the threshold for predicting "Win"  when the probability is more than to 0.48. We can see from the ROC curve that there is little tradeoff between tpr and fpr because the classifcation error rate is so low. The Area under the Curve is 99.7%. After setting the threshold value at 0.48, the test error rate for the Logistic regression is 1.88% which shows that our model is generalizing well to the test data, and not overfitting. 
 
``` {r Data Analysis Log Regression, echo = FALSE}
pred <- prediction(as.vector(predict.glm), as.vector(Test$Win))
perf <- performance(pred, measure = 'tpr', x.measure = 'fpr')

plot(perf, col=2, lwd=3, main="ROC curve")
abline(0,1)

auc = performance(pred, "auc")@y.values
# auc

# FPR
fpr = performance(pred, "fpr")@y.values[[1]]
cutoff = performance(pred, "fpr")@x.values[[1]]
# FNR
fnr = performance(pred,"fnr")@y.values[[1]]

# Plot
rate = as.data.frame(cbind(Cutoff=cutoff, FPR=fpr, FNR=fnr))
rate$distance = sqrt((rate[,2])^2+(rate[,3])^2)

index = which.min(rate$distance)
best = rate$Cutoff[index]
# best

# Plot
matplot(cutoff, cbind(fpr,fnr), type="l",lwd=2, xlab="Threshold",ylab="Error Rate")
# Add legend to the plot
legend(0.35, 1, legend=c("False Positive Rate","False Negative Rate"),
col=c(1,2), lty=c(1,2))

# Add the best value
abline(v=best, col=3, lty=3, lwd=3)

print('Test Classification error for Logistic Regression')
glm.conf <- table(pred = as.factor(ifelse(predict.glm<= 0.48, 0,1)),observed = TestY$Win)
1 - sum(diag(glm.conf))/sum(glm.conf)
```

## Random Forests Model Building
Using the randomForest() function, I fit a Random Forest model to my training data. We can see that the OOB estimate of error rate is 1.48%, which is a small improvement over the Logistic Regression validation error rate of 1.51%, and indicates that our model should not overfit and should generalize well to real data. The False Negative rate is 1.47% while the False Positive rate is 1.49%.

```{r Method Random Forest,echo=FALSE, fig.height = 3, fig.width = 5}
set.seed(8)
rf.HighElo <- randomForest(Win~.-MinGameDuration, data=Train,ntree=250,importance=TRUE,mtry = 3)

print('Summary of rf.HighElo')
rf.HighElo

# Tune mtry parameter
set.seed(9)
rf.tune <- tuneRF(Train,Train$Win,mtryStart = 3, ntreeTry = 50,stepFactor = 1,improve = 0.001, plot = TRUE, doBest = TRUE, trace = FALSE)

```

I used the tuneRF() function to find an optimal value of mtry. Starting with the default of $mtry = \sqrt{p} = 3$, it looks 1 step in each direction to see if it offers enough of an improvement in OOB estimate of error. After running this function, the optimal value was 3.
## Random Forests Model Analysis

Looking at the variable importance plot and saw that GoldDiffPerMin was by far the most important variable in terms of both model accuracy and Gini index. WardPlaceDiffPerMin was the second most important in terms of model accuracy, while ObjectDmgDiffPerMin was second most important in terms of Gini index. Looking at the plot of the number of trees vs the OOB estimate of error rate, we can see that the OOB estimate of error rate stabilizes after about 75 trees and futher performance increases are minimal.
```{r Data Analysis RandomForest, echo=FALSE}

varImpPlot(rf.HighElo,sort=T,main='Variable Importance for rf.HighElo')

plot(rf.HighElo)
legend("top", colnames(rf.HighElo$err.rate),col=1:4,cex=0.8,fill=1:4)


print('Test Classification Error rate for Random Forest')
yhat.rf <- predict(rf.HighElo,newdata = Test)
rf.err = table(pred = yhat.rf, observed = Test$Win)
1 - sum(diag(rf.err))/sum(rf.err)


```

## Final Model
After comparing the validation error for the Logistic Regression model and the Out-Of-Bag error for the Random Forest Model, I choose the Random Forest Model for its higher accuracy. The final Random Forest model as a test error rate of 1.49%.

# IV. Conclusion
In conclusion, classifying the winning League of Legends team can be done well and it depends highly on how much gold a team has, how many more objectives a team does, and how many wards (which provide map vision) a team places. My model only classified 549 observations incorrectly out of 37,114 observations in the test set. However, I recommend for future projects to consider data that captures the state of the game at different poitns of time, to gain an idea of which variables are most beneficial to a team at different points of the game flow.  My final model was 

\begin{align*}
randomForest(Win \sim GoldDiffPerMin + WardDiffPermin + DamageDiffPerMin + \\ 
HealDiffPerMin | ObjectDmgDiffPerMin + FirstTower + \\ 
Firstbaron + FirstInhibitor + FirstDragon)\\
\end{align*}

## Limitations
The limitations of this study are that to win a game in League of Legends, a team has to win the final teamfight and kill most of the enemy team. Both nexus towers are usually destroyed right after this crucial teamfight, along with the nexus. In LoL, each tower destroyed grants global gold to the team. Therefore, it makes sense that GoldDiffPerMin would be such a strong predictor of our response variable, as this could result in a higher GoldDiffPerMin for the winning team. Another Limitation of this study is that it considers the top 0.1% of League of Legends players. These high-ranked players make much fewer mistakes than lower-ranked players. Therefore, they will rarely throwaway games where they have generated an advantage. This explains why are classifier is so good at classifygin "Win" or "Lose", because generally generating a team advantage results in a win, reflecting the skill level of the best League of Legends players.

## Future Research Directions
A similar project showed accuracy rates for end-of-=game data, with an accuracy rate of 98.09% for the logistic regression model and 97.77% for the Random Forest Model [@Medium]. Therefore, I know my results are sound. Future research directions should pursue the impact of champion selection on winning a game, for example, predicting whether a team will win or lose based on the pre-game champion selection. In the competitive League of Legends scene, emphasis is placed on champion synergies and counters. Assuming equal matchmaking, it would be interesting to see how the result of the game is decided before the game has even started. 

# V. References

# VI. Appendix

```{r eval = FALSE}
# How the new features were created

# Feature Creation for Blue and Red Training sets
BlueTrain <- mutate(BlueTrain, GoldDiffPerMin = (blueTotalGold - 
                                                   redTotalGold)/MinGameDuration,
                    WardPlaceDiffPerMin = (blueWardPlaced - 
                                             redWardPlaced)/MinGameDuration,
                    DamageDiffPerMin = (blueChampionDamageDealt - 
                                          redChampionDamageDealt)/MinGameDuration,
                    HealDiffPerMin = (blueTotalHeal - 
                                        redTotalHeal)/MinGameDuration,
                    ObjectDmgDiffPerMin = (blueObjectDamageDealt - 
                                             redObjectDamageDealt)/MinGameDuration,
                    FirstTower = blueFirstTower,
                    FirstBaron = blueFirstBaron,
                    FirstInhibitor = blueFirstInhibitor,
                    FirstDragon = blueFirstDragon,
                    Win = blueWins)
RedTrain <- mutate(RedTrain, GoldDiffPerMin = (redTotalGold - 
                                                 blueTotalGold)/MinGameDuration,
                    WardPlaceDiffPerMin = (redWardPlaced - 
                                             blueWardPlaced)/MinGameDuration,
                    DamageDiffPerMin = (redChampionDamageDealt - 
                                          blueChampionDamageDealt)/MinGameDuration,
                    HealDiffPerMin = (redTotalHeal - 
                                        blueTotalHeal)/MinGameDuration,
                    ObjectDmgDiffPerMin = (redObjectDamageDealt - 
                                             blueObjectDamageDealt)/MinGameDuration,
                    FirstTower = redFirstTower,
                    FirstBaron = redFirstBaron,
                    FirstInhibitor = redFirstInhibitor,
                    FirstDragon = redFirstDragon,
                    Win = redWins)

BlueTest <- mutate(BlueTest, GoldDiffPerMin = (blueTotalGold - 
                                                 redTotalGold)/MinGameDuration,
                    WardPlaceDiffPerMin = (blueWardPlaced - 
                                             redWardPlaced)/MinGameDuration,
                    DamageDiffPerMin = (blueChampionDamageDealt - 
                                          redChampionDamageDealt)/MinGameDuration,
                    HealDiffPerMin = (blueTotalHeal - 
                                        redTotalHeal)/MinGameDuration,
                    ObjectDmgDiffPerMin = (blueObjectDamageDealt - 
                                             redObjectDamageDealt)/MinGameDuration,
                    FirstTower = blueFirstTower,
                    FirstBaron = blueFirstBaron,
                    FirstInhibitor = blueFirstInhibitor,
                    FirstDragon = blueFirstDragon,
                    Win = blueWins)
RedTest <- mutate(RedTest, GoldDiffPerMin = (redTotalGold - 
                                               blueTotalGold)/MinGameDuration,
                    WardPlaceDiffPerMin = (redWardPlaced - 
                                             blueWardPlaced)/MinGameDuration,
                    DamageDiffPerMin = (redChampionDamageDealt - 
                                          blueChampionDamageDealt)/MinGameDuration,
                    HealDiffPerMin = (redTotalHeal - 
                                        blueTotalHeal)/MinGameDuration,
                    ObjectDmgDiffPerMin = (redObjectDamageDealt - 
                                             blueObjectDamageDealt)/MinGameDuration,
                    FirstTower = redFirstTower,
                    FirstBaron = redFirstBaron,
                    FirstInhibitor = redFirstInhibitor,
                    FirstDragon = redFirstDragon,
                    Win = redWins)
```

